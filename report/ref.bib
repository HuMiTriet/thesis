
@misc{wiki_token_ring,
  title = {Token Ring Wikipedia},
  howpublished = {\url{https://en.wikipedia.org/wiki/Token_Ring}},
  author = {Wikipedia},
  year = {2023},
  note = {Accessed on May 24, 2023},
}

@misc{flask,
  title = {Flask web framework},
  howpublished = {\url{https://flask.palletsprojects.com/en/2.3.x/}},
  note = {Accessed on June 04, 2023},
}

@misc{flask_dev_server,
  title = {Flask development server},
  howpublished = {\url{https://flask.palletsprojects.com/en/2.3.x/server/}},
  note = {Accessed on June 06, 2023},
}

@inproceedings{happy_path,
  author = {Edwards, Stephen H. and Shams, Zalia},
  title = {Do Student Programmers All Tend to Write the Same Software Tests?},
  year = {2014},
  isbn = {9781450328333},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2591708.2591757},
  doi = {10.1145/2591708.2591757},
  abstract = {While many educators have added software testing practices to
              their programming assignments, assessing the effectiveness of
              student-written tests using statement coverage or branch coverage
              has limitations. While researchers have begun investigating
              alternative approaches to assessing student-written tests, this
              paper reports on an investigation of the quality of student written
              tests in terms of the number of authentic, human-written defects
              those tests can detect. An experiment was conducted using 101
              programs written for a CS2 data structures assignment where
              students implemented a queue two ways, using both an array-based
              and a link-based representation. Students were required to write
              their own software tests and graded in part on the branch coverage
              they achieved. Using techniques from prior work, we were able to
              approximate the number of bugs present in the collection of student
              solutions, and identify which of these were detected by each
              student-written test suite. The results indicate that, while
              students achieved an average branch coverage of 95.4\% on their own
              solutions, their test suites were only able to detect an average of
              13.6\% of the faults present in the entire program population.
              Further, there was a high degree of similarity among 90\% of the
              student test suites. Analysis of the suites suggest that students
              were following naïve, "happy path" testing, writing basic test
              cases covering mainstream expected behavior rather than writing
              tests designed to detect hidden bugs. These results suggest that
              educators should strive to reinforce test design techniques
              intended to find bugs, rather than simply confirming that features
              work as expected. },
  booktitle = {Proceedings of the 2014 Conference on Innovation \& Technology in
               Computer Science Education},
  pages = {171–176},
  numpages = {6},
  keywords = {test coverage, test quality, happy path, program assignments,
              software testing, automated assessment, automated grading, mutation
              testing},
  location = {Uppsala, Sweden},
  series = {ITiCSE '14},
}

@article{OG_fuzzy_testing,
  author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title = {An Empirical Study of the Reliability of UNIX Utilities},
  year = {1990},
  issue_date = {Dec. 1990},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {33},
  number = {12},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/96267.96279},
  doi = {10.1145/96267.96279},
  abstract = {The following section describes the tools we built to test the
              utilities. These tools include the fuzz (random character)
              generator, ptyjig (to test interactive utilities), and scripts to
              automate the testing process. Next, we will describe the tests we
              performed, giving the types of input we presented to the utilities.
              Results from the tests will follow along with an analysis of the
              results, including identification and classification of the program
              bugs that caused the crashes. The final section presents concluding
              remarks, including suggestions for avoiding the types of problems
              detected by our study and some commentary on the bugs we found. We
              include an Appendix with the user manual pages for fuzz and ptyjig.
              },
  journal = {Commun. ACM},
  month = {dec},
  pages = {32–44},
  numpages = {13},
}

@inproceedings{quickcheck,
  author = {Claessen, Koen and Hughes, John},
  title = {QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs
           },
  year = {2000},
  isbn = {1581132026},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/351240.351266},
  doi = {10.1145/351240.351266},
  abstract = {Quick Check is a tool which aids the Haskell programmer in
              formulating and testing properties of programs. Properties are
              described as Haskell functions, and can be automatically tested on
              random input, but it is also possible to define custom test data
              generators. We present a number of case studies, in which the tool
              was successfully used, and also point out some pitfalls to avoid.
              Random testing is especially suitable for functional programs
              because properties can be stated at a fine grain. When a function
              is built from separately tested components, then random testing
              suffices to obtain good coverage of the definition under test.},
  booktitle = {Proceedings of the Fifth ACM SIGPLAN International Conference on
               Functional Programming},
  pages = {268–279},
  numpages = {12},
  series = {ICFP '00},
}
