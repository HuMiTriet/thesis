\section{Prototyping Phase}

The initial half of the three-month internship was dedicated to learning about distributed systems and developing the prototype.

\subsection{Prototype Architecture}

The prototype was constructed utilizing the Python programming language, complemented by Flask \cite{flask}, a widely used microframework. Flask, built on the foundation of the Werkzeug WSGI toolkit, the Jinja template engine, and the Click CLI toolkit, is classified as a microframework because it does not natively offer many of the features typically required in web application development. 

This lean approach is ideal for the internship project as our goal was to create a functional prototype of communicating nodes, devoid of any graphical user interface.

The figure \ref{fig:prototype_architecture} presents an overview of the prototype, which comprises a minimum of four client nodes, a single proxy, a database or resource manager, and a logger.

\begin{figure}[htbp]
  \centering
  \includesvg{images/prototype_architecture.svg} 
  \caption{Overview of Prototype Architecture}
  \label{fig:prototype_architecture}
\end{figure}

The depiction in figure \ref{fig:prototype_architecture} represents the final version of the prototype. During this stage of the internship, the prototype was built incrementally - we first integrated the clients and resources, followed by the proxy, and lastly the logger.

Each client is equipped with a RESTful API layer that facilitates communication and data exchange between them using HTTP, with JSON serving as the data encoding format.

\subsubsection{Client}

Given that the entire prototype operates on a single localhost, each client is effectively an individual process executing concurrently with others.

Each client features an HTTP endpoint responding to specific HTTP verbs such as
GET, PUT, or POST, etc. In Flask, this is referred to as a view:

\begin{listing}[!ht]
  \begin{minted}{python}
  @bp.route("/<string:resource_id>/request", methods=["POST"])
  async def request_resource(resource_id: str) -> tuple[str, int]:
      data = request.get_json()
      delay_time: str = data["delay_time"]
      # ...
  \end{minted}
\caption{Function signature of a client}
\label{code:signature}
\end{listing}

As visible from code snippet \ref{code:signature}, each view is prefaced by the 'async' keyword, signifying that it's an asynchronous function. 

The 'async' keyword  enable the usage of 'await' for handling
scenarios where multiple messages need to be sent at the same time. 
If this were to be performed without concurrency, then the process has to send 
requests sequentially one after another resulting in higher delay.


\begin{listing}[!ht]
\begin{minted}{python}
coroutines = []

for target_url in CLIENT_URL:
    coroutine = session.post(
        # ...
        proxy=PROXY_URL,
    )
    coroutines.append(coroutine)

await asyncio.gather(*coroutines)
\end{minted}
\caption{Simultaneous dispatch of multiple requests}
\label{code:multiple_requests}
\end{listing}

The code in \ref{code:multiple_requests} packages all the coroutines responsible for sending HTTP requests into an array. Using 'asyncio.gather()', the system waits for all coroutines and completes when each coroutine has finished execution.

\subsubsection{Proxy}

The primary role of the proxy is to facilitate fault injection into the system and to monitor and generate data for further analysis.

As evident in the \ref{code:multiple_requests} snippet, there is the line \textit{proxy=PROXY\_URL}. Similarly, for all HTTP requests and responses in the prototype, this line is part of the options when sending HTTP requests, to ensure all communication is routed via the proxy.

The integration of this option in sending HTTP requests allows for fault injection. For additional information on fault injection and its significance, please refer to \ref{subsec:fault_injection}.

In its early stages, the proxy was executed using the Flask default development server \cite{flask_dev_server}.

However, the clients were communicating using aiohttp, sending all messages asynchronously, essentially dispatching requests more or less simultaneously. 
In contrast, the initial proxy, operating on Flask's development server, was single-threaded. 
Consequently, requests were sent sequentially, one after another, due to the proxy's inability to handle concurrent requests. 
The proxy has become a bottleneck that affected the correctness of the result.

To overcome this issue, we transitioned from the development server to Gunicorn,
which allow for the use of multiple workers concurrently for the proxy.

\begin{listing}[!ht]
  \begin{minted}{bash}
gunicorn --reload -w 4 -b 127.0.0.1:5001 "proxy:create_app()"
  \end{minted}
  \caption{Execution of Gunicorn with 4 workers}
\end{listing}

Now, with the proxy being served by at least four workers, one for each client, 
each client's HTTP request is being sent concurrently, aligning with the initial intention.

\subsubsection{Logger}

The logger is a continuously running process, essential for data acquisition. This process is equipped with two endpoints:

The first endpoint is the log endpoint:

\begin{listing}[H]
  \begin{minted}{python}
  @app.route("/<string:resource_id>/log", methods=["PUT"])
  def log(resource_id: str):
  \end{minted}
  \caption{Code snippet showcasing the logger's log endpoint}
  \label{code:logger_log}
\end{listing}

The endpoint as highlighted in \ref{code:logger_log} is triggered at the initiation of a client request, which indicates the client node's initial intent to access the resource, and it's activated again when the resource has been obtained by the client. 

Each client request is accompanied by a Unix timestamp, indicating the exact time at which the request was made.

At the first logger call, the start time of the request is recorded:

\begin{listing}[!ht]
  \begin{minted}{python}
   latency_dict: dict[str, float] = {}
  \end{minted}
  \caption{An in-memory dictionary to store latency}
  \label{code:latency_dict}
\end{listing}

When the log endpoint is triggered for the second time, the logger will subtract the time stored in the \textit{latency\_dict} from the time the second request was made, providing the latency.

\begin{listing}[!ht]
  \begin{minted}{python}
  latency_tally: list[dict[str, float]] = []
  \end{minted}
  \caption{Latency results are stored in a list}
  \label{code:latency_tally}
\end{listing}

\textit{latency\_tally} is a list of dictionaries. Each dictionary's keys represent the name of the client, for instance, client\_1, and the corresponding value, such as 0.02, signifies the client's latency.


\subsubsection{Resource Manager}
\label{subsubsec:resource_manager}

Resource manager is another process where the client contact to either lock or
delete a particular resource.

\begin{listing}[!ht]
  \begin{minted}{python}
@bp.route("/<string:resource_id>/lock", methods=["PUT"])
def lock(resource_id: str):
  # ...

@bp.route("/<string:resource_id>/lock", methods=["DELETE"])
def unlock(resource_id: str):
  # ...
  \end{minted}
\end{listing}

In addition, there is also another endpoint to check whether a race condition 
has occurred:

\begin{listing}[H]
  \begin{minted}{python}
@bp.route("/<string:resource_id>/is_lock", methods=["GET"])
def check_locking_status(resource_id: str):
  # ...
  \end{minted}
\end{listing}

All the resources are stored in memory using a class 

\begin{listing}[!ht]
  \begin{minted}{python}
@dataclass(slots=True)
class ServerState:
    race_condition: bool = False
    resource: dict[str, bool] = field(default_factory=dict[str, bool])
  \end{minted}
\end{listing}


